{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pickle\n",
    "import numpy as np\n",
    "import h5py\n",
    "import argparse\n",
    "import sys\n",
    "import re\n",
    "import codecs\n",
    "from itertools import izip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "# Converts indices to words \n",
    "def convert_to_words(indices, indices_to_word):\n",
    "    return (' '.join([indices_to_word[ind] for ind in indices]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "directory = '../data/MovieTriples/'\n",
    "\n",
    "# Loading all the possible files into memory\n",
    "with open(directory + 'Training.triples.pkl') as f:\n",
    "    train_set = pickle.load(f)\n",
    "    \n",
    "with open(directory + 'Validation.triples.pkl') as f:\n",
    "    valid_set = pickle.load(f)\n",
    "    \n",
    "with open(directory + 'Test.triples.pkl') as f:\n",
    "    test_set = pickle.load(f)\n",
    "    \n",
    "with open(directory + 'Word2Vec_WordEmb.pkl') as f:\n",
    "    emb_wordvec = pickle.load(f)\n",
    "    \n",
    "with open(directory + 'MT_WordEmb.pkl') as f:\n",
    "    emb_mt = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('<s>', 1, 588827, 785135), 90)\n",
      "(('</s>', 2, 588827, 785135), 378)\n",
      "(('.', 3, 855616, 192250), 426)\n",
      "(('<unk>', 0, 190588, 89059), 604)\n",
      "((\"'\", 4, 457542, 160249), 2115)\n"
     ]
    }
   ],
   "source": [
    "# Implement the word indices according to the format for the seq2seq model\n",
    "\n",
    "# Make sure that the word_indices are 1 indexed for lua\n",
    "\n",
    "# Do a swap with the embeddings and word_indices so it follows the conventions for indices \n",
    "# self.d = {self.PAD: 1, self.UNK: 2, self.BOS: 3, self.EOS: 4}\n",
    "\n",
    "with open(directory + 'Training.dict.pkl') as f:\n",
    "    word_mappings = pickle.load(f)\n",
    "\n",
    "# All the swaps necessary to make the formatting consistent with seq2seq (sorry, it's so messy)\n",
    "del_ind = []\n",
    "for i in range(len(word_mappings)):\n",
    "    word_mapping = word_mappings[i]\n",
    "    if word_mapping[0] == '<unk>' or word_mapping[0] == '<s>' or \\\n",
    "        word_mapping[0] == '</s>' or word_mapping[0] == '.' or \\\n",
    "        word_mapping[0] == \"'\":\n",
    "            print(word_mapping, i)\n",
    "            del_ind.append(i)\n",
    "\n",
    "del_ind.sort(reverse=True)\n",
    "for ind in del_ind:\n",
    "    del word_mappings[ind]\n",
    "        \n",
    "word_mappings.append(('<blank>', 1, 0, 0))\n",
    "word_mappings.append(('<unk>', 2, 190588, 89059))\n",
    "word_mappings.append(('<s>', 3, 588827, 785135))\n",
    "word_mappings.append(('</s>', 4, 588827, 785135))\n",
    "word_mappings.append(('.', 10003, 855616, 192250))\n",
    "word_mappings.append((\"'\", 10004, 457542, 160249))\n",
    "word_mappings.append(('<t>', 10005, 0, 0))\n",
    "\n",
    "# Sanity check\n",
    "check_mappings = range(1, len(word_mappings)+1)\n",
    "for word_mapping in word_mappings:\n",
    "    check_mappings.remove(word_mapping[1])\n",
    "assert check_mappings == []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 6, 1577, 11, 22, 52, 300, 413, 28, 2, 1, 5433, 28, 497, 22, 308, 121, 28, 190, 3, 2, 1, 43, 7, 112, 194, 6, 27, 90, 5, 8, 9, 2]\n",
      "[3, 6, 1577, 11, 22, 52, 300, 413, 28, 4, 3, 5433, 28, 497, 22, 308, 121, 28, 190, 10003, 4, 3, 43, 7, 112, 194, 6, 27, 90, 5, 8, 9, 4]\n"
     ]
    }
   ],
   "source": [
    "# The changes that need to occur in the actual text examples are: \n",
    "# ., 3 -> 10003\n",
    "# ', 4 -> 10004\n",
    "# <unk>, 0 -> 2\n",
    "# <s>, 1 -> 3\n",
    "# </s>, 2 -> 4\n",
    "\n",
    "print(train_set[0])\n",
    "data_sets = [train_set, valid_set, test_set]\n",
    "for i in range(len(data_sets)):\n",
    "    for j in range(len(data_sets[i])):\n",
    "        line = data_sets[i][j]\n",
    "        for k in range(len(line)):\n",
    "            ind = line[k]\n",
    "            if ind == 3:\n",
    "                line[k] = 10003\n",
    "            elif ind == 4:\n",
    "                line[k] = 10004\n",
    "            elif ind == 0:\n",
    "                line[k] = 2\n",
    "            elif ind == 1:\n",
    "                line[k] = 3\n",
    "            elif ind == 2:\n",
    "                line[k] = 4\n",
    "        data_sets[i][j] = line\n",
    "print(train_set[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10005\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('raining', 4959, 53, 48),\n",
       " ('writings', 9977, 18, 15),\n",
       " ('yellow', 2155, 175, 142),\n",
       " ('four', 341, 2299, 2081),\n",
       " ('prices', 5660, 43, 40)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Not entirely sure what the other two numbers reprsent in the word index table\n",
    "# Maybe corresponds to the counts in train... or something?\n",
    "\n",
    "print(len(word_mappings))\n",
    "word_mappings[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Move through the list of words and indices and generate a dictionary\n",
    "# matching the indices to words\n",
    "\n",
    "# indices -> word\n",
    "indices_to_word = {}\n",
    "for word_ex in word_mappings: \n",
    "    indices_to_word[word_ex[1]] = word_ex[0]\n",
    "    \n",
    "# word -> indices\n",
    "word_to_indices = {}\n",
    "for word_ex in word_mappings: \n",
    "    word_to_indices[word_ex[0]] = word_ex[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> you lied to me so many times -- </s> <s> reggie -- trust me once more -- please . \n",
      " can i really believe you this time , <person> ? </s>\n"
     ]
    }
   ],
   "source": [
    "# It looks like the </s> <s> denotes different speakers\n",
    "# We want to break out the first to examples and then generate the \n",
    "# third as output\n",
    "\n",
    "# For now we can join the first two sentences and assume that the encoder will figure it out with the </s><s>\n",
    "# Afterwards, we can think about ways to incorporate the three uttterances\n",
    "\n",
    "line = ' '.join([indices_to_word[ind] for ind in train_set[0]])\n",
    "line = line.split('</s> <s>')\n",
    "context = line[0] + '</s> <s>' + line[1]\n",
    "output = line[2]\n",
    "\n",
    "# So our input would be\n",
    "print(context)\n",
    "# And our output would be\n",
    "print(output)\n",
    "\n",
    "# I'll now generate matrices with that format for the rest of the data. \n",
    "# Everything will be padded with a 10003 character at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> you lied to me so many times -- </s> <s> reggie -- trust me once more -- please . </s> <s> can i really believe you this time , <person> ? </s>'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line = ' '.join([indices_to_word[ind] for ind in train_set[0]])\n",
    "line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> you lied to me so many times -- </s> <s> reggie -- trust me once more -- please .\n",
      "can i really believe you this time , <person> ? </s>\n"
     ]
    }
   ],
   "source": [
    "pattern = [word_to_indices['</s>'], word_to_indices['<s>']]\n",
    "\n",
    "for ind in range(len(train_set[0]))[::-1]:\n",
    "    if pattern == train_set[0][ind:ind+2]:\n",
    "        break_pt = ind\n",
    "        break\n",
    "        \n",
    "context = train_set[0][:break_pt]\n",
    "output = train_set[0][break_pt+2:]\n",
    "\n",
    "print(convert_to_words(context, indices_to_word))\n",
    "print(convert_to_words(output, indices_to_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127753\n",
      "31938\n"
     ]
    }
   ],
   "source": [
    "# Apply above basic parsing to all contexts and outputs\n",
    "\n",
    "PADDING = word_to_indices['<blank>']\n",
    "END_OF_CONV = word_to_indices['<t>']\n",
    "\n",
    "full_context = []\n",
    "full_output = []\n",
    "max_len_context = 0\n",
    "max_len_output = 0 \n",
    "\n",
    "for i in range(len(train_set)):\n",
    "    break_pt = []\n",
    "    for ind in range(len(train_set[i]))[::-1]:\n",
    "        if pattern == train_set[i][ind:ind+2]:\n",
    "            break_pt.append(ind)\n",
    "\n",
    "    context = train_set[i][:break_pt[0]]\n",
    "    output = train_set[i][break_pt[0]+2:]\n",
    "    \n",
    "    context = context + [word_to_indices['</s>']]\n",
    "    output = [word_to_indices['<s>']] + output\n",
    "    \n",
    "    # Start of sentence and end of sentence is ONLY used at the end\n",
    "    # We create a new character that represents the start and end of a conversation\n",
    "    context = context[:break_pt[1]] + [END_OF_CONV] + context[break_pt[1]+2:]\n",
    "    \n",
    "    \n",
    "    # Cap the target and src length at 302 words to make computation simpler, goes up to ~1500\n",
    "    if len(context) > 52:\n",
    "        continue\n",
    "    if len(output) > 52:\n",
    "        continue\n",
    "    \n",
    "    max_len_output = max(max_len_output, len(output))\n",
    "    max_len_context = max(max_len_context, len(context))\n",
    "    max_len_output = 52\n",
    "    max_len_context = 52\n",
    "        \n",
    "    full_context.append(context)\n",
    "    full_output.append(output)\n",
    "    \n",
    "# Add padding to all contexts and outputs\n",
    "for i in range(len(full_context)):\n",
    "    full_context[i] = full_context[i] + [PADDING] * (max_len_context - len(full_context[i]))\n",
    "    full_output[i] = full_output[i] + [PADDING] * (max_len_output - len(full_output[i]))\n",
    "    \n",
    "full_context = (full_context)\n",
    "full_output = (full_output)\n",
    "\n",
    "# TODO: split randomly rather than at set index\n",
    "# NB: this data is already shuffled so it's not _that_ big a deal\n",
    "ind = int(0.8*len(full_context))\n",
    "train_full_context = full_context[:ind]\n",
    "train_full_output = full_output[:ind]\n",
    "valid_full_context = full_context[ind+1:]\n",
    "valid_full_output = full_output[ind+1:]\n",
    "\n",
    "# Create micro datasets for quick n dirty code checks\n",
    "m_ind = 5000\n",
    "train_micro_context = full_context[:m_ind]\n",
    "train_micro_output = full_output[:m_ind]\n",
    "valid_micro_context = full_context[m_ind+1:m_ind+1+(m_ind/10)]\n",
    "valid_micro_output = full_output[m_ind+1:m_ind+1+(m_ind/10)]\n",
    "\n",
    "print(len(train_full_context))\n",
    "print(len(valid_full_context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is super inefficient, put it together last minute. Don't judge :)\n",
    "def write_context_to_file(filename, full_context):\n",
    "    f = open(filename, 'w')\n",
    "    for context in full_context: \n",
    "        for ind in context:\n",
    "            f.write(str(ind) + ' ')\n",
    "        f.write('\\n')\n",
    "    f.close()\n",
    "    \n",
    "# Full training/validation\n",
    "write_context_to_file('../seq2seq-attn/data/train_full_context.txt', train_full_context)\n",
    "write_context_to_file('../seq2seq-attn/data/train_full_output.txt', train_full_output)\n",
    "write_context_to_file('../seq2seq-attn/data/dev_full_context.txt', valid_full_context)\n",
    "write_context_to_file('../seq2seq-attn/data/dev_full_output.txt', valid_full_output)\n",
    "\n",
    "# Micro training/validation\n",
    "write_context_to_file('../seq2seq-attn/data/train_micro_context.txt', train_micro_context)\n",
    "write_context_to_file('../seq2seq-attn/data/train_micro_output.txt', train_micro_output)\n",
    "write_context_to_file('../seq2seq-attn/data/dev_micro_context.txt', valid_micro_context)\n",
    "write_context_to_file('../seq2seq-attn/data/dev_micro_output.txt', valid_micro_output)\n",
    "\n",
    "with open('../seq2seq-attn/data/targ.dict', 'w') as f:\n",
    "    for i in range(1, len(indices_to_word)+1):\n",
    "        f.write(indices_to_word[i] + ' ' + str(i) + '\\n')\n",
    "        \n",
    "with open('../seq2seq-attn/data/src.dict', 'w') as f:\n",
    "    for i in range(1, len(indices_to_word)+1):\n",
    "        f.write(indices_to_word[i] + ' ' + str(i) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Embeddings map to the generated word_dict \n",
    "# print(emb_wordvec)\n",
    "# print(emb_wordvec[0].shape)\n",
    "# print(emb_mt)\n",
    "# print(emb_mt[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
