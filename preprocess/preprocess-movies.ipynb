{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pickle\n",
    "import numpy as np\n",
    "import h5py\n",
    "import argparse\n",
    "import sys\n",
    "import re\n",
    "import codecs\n",
    "from itertools import izip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "# Converts indices to words \n",
    "def convert_to_words(indices, indices_to_word):\n",
    "    return (' '.join([indices_to_word[ind] for ind in indices]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "directory = '../data/MovieTriple/'\n",
    "\n",
    "# Loading all the possible files into memory\n",
    "with open(directory + 'Training.triples.pkl') as f:\n",
    "    train_set = pickle.load(f)\n",
    "    \n",
    "with open(directory + 'Validation.triples.pkl') as f:\n",
    "    valid_set = pickle.load(f)\n",
    "    \n",
    "with open(directory + 'Test.triples.pkl') as f:\n",
    "    test_set = pickle.load(f)\n",
    "\n",
    "with open(directory + 'Training.dict.pkl') as f:\n",
    "    word_mappings = pickle.load(f)\n",
    "    \n",
    "with open(directory + 'Word2Vec_WordEmb.pkl') as f:\n",
    "    emb_wordvec = pickle.load(f)\n",
    "    \n",
    "with open(directory + 'MT_WordEmb.pkl') as f:\n",
    "    emb_mt = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10003\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('raining', 4959, 53, 48),\n",
       " ('writings', 9977, 18, 15),\n",
       " ('yellow', 2155, 175, 142),\n",
       " ('four', 341, 2299, 2081),\n",
       " ('prices', 5660, 43, 40)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training.dict.pkl: Dictionary with 10000 words extracted from the training \n",
    "# set (Training_Shuffled_Dataset.txt). These terms represent 97.97% of the \n",
    "# entire training set.\n",
    "\n",
    "# Not entirely sure what the other two numbers reprsent in the word index table\n",
    "# Maybe corresponds to the counts in train... or something?\n",
    "\n",
    "print(len(word_mappings))\n",
    "word_mappings[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Move through the list of words and indices and generate a dictionary\n",
    "# matching the indices to words\n",
    "\n",
    "# indices -> word\n",
    "indices_to_word = {}\n",
    "for word_ex in word_mappings: \n",
    "    indices_to_word[word_ex[1]] = word_ex[0]\n",
    "    \n",
    "# word -> indices\n",
    "word_to_indices = {}\n",
    "for word_ex in word_mappings: \n",
    "    word_to_indices[word_ex[0]] = word_ex[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> you lied to me so many times -- </s> <s> reggie -- trust me once more -- please . </s> <s> can i really believe you this time , <person> ? </s>\n",
      "<s> you lied to me so many times -- </s> <s> reggie -- trust me once more -- please . \n",
      " can i really believe you this time , <person> ? </s>\n"
     ]
    }
   ],
   "source": [
    "# It looks like the </s> <s> denotes different speakers\n",
    "# We want to break out the first to examples and then generate the \n",
    "# third as output\n",
    "print(convert_to_words(train_set[0], indices_to_word))\n",
    "\n",
    "# For now we can join the first two sentences and assume that the encoder will figure it out with the </s><s>\n",
    "# Afterwards, we can think about ways to incorporate the three uttterances\n",
    "\n",
    "line = ' '.join([indices_to_word[ind] for ind in train_set[0]])\n",
    "line = line.split('</s> <s>')\n",
    "context = line[0] + '</s> <s>' + line[1] \n",
    "output = line[2]\n",
    "\n",
    "# So our input would be\n",
    "print(context)\n",
    "# And our output would be\n",
    "print(output)\n",
    "\n",
    "# I'll now generate matrices with that format for the rest of the data. \n",
    "# Everything will be padded with a 10003 character at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> you lied to me so many times -- </s> <s> reggie -- trust me once more -- please .\n",
      "can i really believe you this time , <person> ? </s>\n"
     ]
    }
   ],
   "source": [
    "pattern = [word_to_indices['</s>'], word_to_indices['<s>']]\n",
    "\n",
    "for ind in range(len(train_set[0]))[::-1]:\n",
    "    if pattern == train_set[0][ind:ind+2]:\n",
    "        break_pt = ind\n",
    "        break\n",
    "        \n",
    "context = train_set[0][:break_pt]\n",
    "output = train_set[0][break_pt+2:]\n",
    "\n",
    "print(convert_to_words(context, indices_to_word))\n",
    "print(convert_to_words(output, indices_to_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(196308, 1500)\n",
      "(196308, 1483)\n"
     ]
    }
   ],
   "source": [
    "# Apply above basic parsing to all contexts and outputs\n",
    "\n",
    "PADDING = 10003\n",
    "full_context = []\n",
    "full_output = []\n",
    "max_len_context = 0\n",
    "max_len_output = 0 \n",
    "\n",
    "for i in range(len(train_set)):\n",
    "    for ind in range(len(train_set[i]))[::-1]:\n",
    "        if pattern == train_set[i][ind:ind+2]:\n",
    "            break_pt = ind\n",
    "            break\n",
    "\n",
    "    context = train_set[i][:break_pt]\n",
    "    output = train_set[i][break_pt+2:]\n",
    "    \n",
    "    max_len_output = max(max_len_output, len(output))\n",
    "    max_len_context = max(max_len_context, len(context))\n",
    "    \n",
    "    full_context.append(context)\n",
    "    full_output.append(output)\n",
    "    \n",
    "# Add padding to all contexts and outputs\n",
    "for i in range(len(full_context)):\n",
    "    full_context[i] = full_context[i] + [PADDING] * (max_len_context - len(full_context[i]))\n",
    "    full_output[i] = full_output[i] + [PADDING] * (max_len_output - len(full_output[i]))\n",
    "    \n",
    "full_context = np.array(full_context)\n",
    "full_output = np.array(full_output)\n",
    "\n",
    "print(full_context.shape)\n",
    "print(full_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[  6.87465274e-04,  -4.67660745e-03,   9.75181500e-03, ...,\n",
      "          6.63852702e-03,   1.36801081e-02,   1.22574021e-02],\n",
      "       [ -1.38690870e-03,  -4.40222603e-03,  -4.37635566e-03, ...,\n",
      "         -7.12008740e-03,  -8.75116355e-03,   1.79811661e-03],\n",
      "       [  1.16901258e-02,  -6.77742948e-03,  -1.96688132e-03, ...,\n",
      "          7.17334130e-03,  -8.84545310e-03,   7.13838460e-03],\n",
      "       ..., \n",
      "       [  1.82406867e-01,  -9.06357709e-01,   8.65739462e-01, ...,\n",
      "          5.20825236e-01,   8.90028319e-01,   1.24629413e+00],\n",
      "       [  1.05338908e-02,  -8.79297778e-01,  -1.47780843e+00, ...,\n",
      "          1.16881122e+00,  -3.22146225e-01,   2.51425509e+00],\n",
      "       [ -4.78275664e-01,   4.13101999e-01,   2.49392604e-01, ...,\n",
      "         -1.25812961e+00,  -1.46552975e+00,   4.08299012e-01]]), array([[ 1.,  1.,  1., ...,  1.,  1.,  1.],\n",
      "       [ 1.,  1.,  1., ...,  1.,  1.,  1.],\n",
      "       [ 1.,  1.,  1., ...,  1.,  1.,  1.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])]\n",
      "(10003, 300)\n",
      "[array([[-0.00284971, -0.01408122,  0.0090998 , ...,  0.01453215,\n",
      "         0.00634608,  0.00964129],\n",
      "       [-0.01565377, -0.00629201, -0.00789775, ..., -0.00426924,\n",
      "        -0.01983844, -0.00329519],\n",
      "       [-0.00637981, -0.00181596, -0.01312016, ...,  0.00553877,\n",
      "        -0.0068264 , -0.00316206],\n",
      "       ..., \n",
      "       [ 0.65919146, -0.25554703,  0.04921132, ...,  0.62415045,\n",
      "         0.01305119, -0.09292043],\n",
      "       [ 0.73499387,  0.05870455,  0.20734325, ...,  0.64578054,\n",
      "         0.12073742,  0.51444995],\n",
      "       [ 0.93033449, -0.46372942,  0.23545632, ...,  0.10020712,\n",
      "        -0.77866588,  0.49508118]]), array([[ 1.,  1.,  1., ...,  1.,  1.,  1.],\n",
      "       [ 1.,  1.,  1., ...,  1.,  1.,  1.],\n",
      "       [ 1.,  1.,  1., ...,  1.,  1.,  1.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])]\n",
      "(10003, 300)\n"
     ]
    }
   ],
   "source": [
    "# Embeddings map to the generated word_dict \n",
    "print(emb_wordvec)\n",
    "print(emb_wordvec[0].shape)\n",
    "print(emb_mt)\n",
    "print(emb_mt[0].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
